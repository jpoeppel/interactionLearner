\chapter{Evaluation\label{chap:evaluation}}


%Explain simulation and communication
%Evaluate different models task based
%How where the realisations/ideas tested?
%What were the results of those tests?

The implementations of the developed concepts were evaluated with regard to the three requirements stated in the introduction: 

\begin{itemize}
\item Update models incrementally during the interaction
\item Prediction of future object states
\item Incrementally reach a given target configuration
\end{itemize}

In order to evaluate these requirements, the implementations are tested in two different tasks using a physics simulation. The first task, the \textit{Push Task Simulation}, tests the forward models of the different implementations. The actual task as well as the results achieved in that task are explained in section \ref{sec:pushTaskSim}.
The second task, \textit{Move to Target}, tests the implementations' ability to move an object towards a given configuration. This task and its results are explained in section \ref{sec:moveToTarget}.
Both tasks are evaluated using different settings for the implementations.

The used simulation software as well as the simulated environment are explained in section \ref{sec:environment}.


\section{Simulation and environment \label{sec:environment}} 

The models are supposed to learn object manipulation through interaction.
The developed prototype implementations are tested in a simplified simulation. For this work gazebo \cite{gazebo} version 2.2.3 was used with the physics engine Open Dynamics Engine (ODE) \cite{ode}.
Gazebo simulates physics one step at a time at fixed time intervals. It is possible to configure the maximum \textit{step size} for each update step as well as the number of steps to be performed in one second in real time (\textit{real time update rate}). 
The simulation time depends on these two values. For this thesis, the standard step size of 0.001 simulation seconds is used. The real time update rate, which determines how many steps are performed in one second in real time, is used to control the speed at which the simulation runs. 
This thesis uses an update rate of 500 when the models are to be updated at 100Hz in simulation time. By setting the update rate to that half of real time, the models have 0.02 seconds in real time in order to finish their updates and queries. %TODO consider redoing or moving elsewhere
The simulation sends information about the objects every x steps, where x depends on the chosen update rate and step size. When using 100Hz, the simulation publishes the objects' information every 10 steps of physics simulation.

A very simple two dimensional environment is used, only containing a spherical actuator and a rectangular cube object that can be seen in figure \ref{fig:gazeboWorld}. %TODO add another object?
The used action primitives were chosen based on their ease of implementation in the simulation, since it is quite easy to change the global velocity of an object
at runtime. For simplicity the spherical actuator's orientation is fixed to 0 so that the global velocities always correspond to the local coordinate system of the actuator.

The dimensions of all objects as well as their friction coefficients used by the physics simulation are listed in table \ref{tab:environmentObjects}.
The fairly high friction coefficient for the block was chosen in order to better suit the assumption of the object state model. By increasing the friction the block is less likely to slide. The high mass for the actuator was chosen so that the used velocities actually move the other object since forces are not set directly.

\begin{table}
	\centering
	\begin{tabular*}{\textwidth}{@{\extracolsep{\fill} } c c c c c}
			\hline \textbf{Object} & \textbf{Dimension} & \multicolumn{2}{c}{\textbf{Friction coefficients}} & Mass \\ 
			\multicolumn{2}{c}{} & $\mu_1$ & $\mu_2$ & \\
			\hline \hline 
			 Actuator & $0.025m$ & 0.01 & 0.01 & 10\si{\kg} \\
			 Blue rectangular block & $0.5m \times 0.1m \times 0.1m$ & 0.9 & 0.9 & 1\si{\kg} \\  
			 Red square block & $0.5m \times 0.5m \times 0.1m$ & 0.9 & 0.9 & 1\si{\kg} \\  
			\hline 
	\end{tabular*} 
	\caption{Table summarizing the dimensions, friction coefficients and masses for the objects in the environment. If only one dimension is given, it represents the radius. Three dimensions correspond to the width, depth and height of an object.}
	\label{tab:environmentObjects}
\end{table}

\begin{figure} 
	\centering
	\includegraphics[width=10cm]{gazeboWorld.png} 
	\caption{Overview of the used environment. The black sphere represents the actuator, the models can control using action primitives. The blue square represents the object that the actuator interacts with.}
	\label{fig:gazeboWorld}
\end{figure}


\subsection{Communication}
Gazebo uses a client-server architecture. The actual simulation is being performed by the server.
This server publishes Google Protobuf \cite{protobuf} messages via TCP/IP. Interested clients, for example a graphical user interface, can register themselves as listener to messages of desired types. 
It is also possible to send messages to the server in order to influence the simulation. 

The server can furthermore be extended by writing custom plug-ins that handle self defined messages or perform some sort of additional computation at each update step of the simulation. 

In order for the Python prototypes to communicate with the simulation, an interface on both sides was created.

On the side of the simulation, a custom server plug-in was written. This plug-in publishes the information described in table \ref{tab:availInformation} in the realization chapter at the fixed rate. 
The information for all objects is packed into a custom Protobuf message, which is explained in details in Appendix \ref{sec:protobufMessages}. Basically, this message simply contains a list of object descriptions where each object description contains the information explained in table \ref{tab:availInformation}.
Furthermore, the plug-in receives custom control messages. These messages allow the Python interface to influence the simulation.
While the exact messages are explained in the Appendix \ref{sec:protobufMessages}, table \ref{tab:commands} gives an overview about the possible commands, that can be send to the simulation.

On the Python side, an interface was written with the help of the module pygazebo \cite{pygazebo}, which provides Python bindings for the message passing system used by gazebo. This interface handles the messages that are received from and send to the simulation. 
At each update step, the interface performs four to five actions:
\begin{enumerate}
\item Construct a suitable worldstate from the provided information
\item Get a prediction about the next worldstate from the model using the current worldstate
\item Update the model with the current worldstate
\item Get an action primitive from the model given a target configuration
\item Send the current action primitive to the simulation
\end{enumerate}

The 4th step is only performed in the tasks evaluating the inverse model. In the \textit{Push Task Simulation} a fixed action primitive is predetermined.

Furthermore, the Python interface provides the testing framework of setting up the models and managing training and testing runs as explained in detail below.

\begin{table}
	\centering
	\begin{tabular}{|c|c|}
		\hline \textbf{Command} & \textbf{Meaning} \\ 
		\hline Move Command & Sets the velocity for the actuator \\ 
		\hline Pause & Pauses the simulation \\
		\hline Continue & Continues the simulation \\
		\hline Reset & Resets all objects to starting configuration \\
		\hline Set Pose & Places a specified object at a certain position and orientation \\
		\hline
	\end{tabular} 
	\caption{Overview of all implemented commands to influence the simulation.}
	\label{tab:commands}
\end{table}

\subsection{Sources of noise}

Although the object's attributes can be read accurately from the simulation, some random noise is still present in the data: 

1) The physics simulation and the sensors that record the objects' attributes are run in different threads within the simulation. These threads cannot always be perfectly synchronized which can result in minor differences in consecutive timesteps. 

2) The physics simulation itself might encounter numerical instabilities, especially when computing resting forces. These instabilities
might lead to small oscillations within an object's features, such as position. In order to filter out such small noise, all
information from the simulation is rounded to 3 decimal places while constructing the worldstate. 

3) The strongest variation comes from the fact that the implementation communicates with the simulation via asynchronous messages. 
All runs start in a configuration where all objects are resting. In the very first update step, the Python interface sends the first
action primitive to the simulation, which results in the actuator moving. The exact time when the simulation retrieves and executes
the action is however not deterministic. Depending on the system's load the simulation might have already performed 5 update steps before
it receives and executes the action. In this case, the action affects only half of the update steps. In another situation, the action might
be executed earlier. From the model's point of view, the same action was performed from the same initial situation, but the resulting changes in object states can differ greatly. 

Since this thesis considers a rather simple scenario and these three sources of noise are already present due to the used technologies,
no additional artificial noise was introduced to the system. Both models needed to be flexible enough to learn from the noisy data without
fixating on outliers.

\section{Push Task Simulation \label{sec:pushTaskSim}}

The \textit{Push Task Simulation} is designed to test the accuracy of the forward model of both concepts. 

\subsection{Scenario description}

In this task, the actuator uses a constant action primitive in order to first move towards the block and then push it.
In order to evaluate different interactions, the actuator starts at different starting positions on a line parallel to the block. 

The distance between the starting line and the center block is 25cm. With the used speed, the actuator reaches the block in about 35 update steps after the start of a run. Depending on the third issue explained above, this number can vary a little.

After 150 update steps, a \textit{run} ends and the objects are reseted. This means, that the block will be placed at its resting position and the actuator is moved back to the starting line. 

During a run, the block status can change in a number of ways. In case the actuator goes past the block, it will not change at all. If it is pushed in the center, it will be translated up to \m{0.6}. Furthermore, depending on where the block is pushed, it will be rotated up to \rad{1.02} (around 58Â°) in either direction. The maximum rotation is experienced when the actuator starts around \m{0.105} away from the origin.

An example starting and end configuration can be seen in figure \ref{fig:pushTaskSim}. 

As the name suggest this task is designed to evaluate the prediction performance of the models during the pushing interaction. The \enquote{simulation} part of the name comes from the fact, that the models will make consecutive predictions based on their last prediction. At the start of each run, the models will make their first prediction based on the current worldstate provided by the simulation. This prediction is then used to construct an appropriate next worldstate which is used at the next update step from the simulation.
Furthermore, the predicted states of both the actuator and the object are send to the simulation for visualization as can be seen in figure \ref{fig:pushTaskSim}.

The models will not get any feedback from the real environment in this setting during testing, which means that they will not be able to adapt their predictions based on the actual interaction. 

In order to be able to make predictions at all, the model will first perform a number of training runs. During these training runs, the models are provided with the current worldstate from the simulation in order to train their local models.

In all runs, an action primitive is used that sets the velocity of the actuator to 0.5m/s straight upwards. Although only a constant global action is used, the prediction task is still challenging enough: \\
The constant action primitive does make it easy for the object state concept to learn the forward model of the actuator. However, the local forward model for the object is not as simple because the velocity is not constant in local coordinates but rather depends on the object's orientation. 
Similarly, in the interaction concept the used action primitive is also transformed to the local coordinate frame of the reference object.

Different kinds of training scenarios are evaluated in this task which are further explained below when their results are presented.


\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{PushTaskSim.png}
	\caption{Exemplary start and end configuration of a \textit{Push Task Simulation} run. The left side shows the start configuration, while on the right the corresponding end configuration can be seen.
		The darker objects (dark blue and black) represent the actual block and actuator, while the transparent objects symbolize the predicted objects.}
	\label{fig:pushTaskSim}
\end{figure}


\subsection{Evaluation criteria}

This task tries to evaluate the precision of the different models. In order to measure this precision, the distance between the predicted object state and the actual object state is computed at the end of each test run.
In this scenario, the models predict up to two quantities: The position and the orientation. 

As already mentioned before, finding a metric that can adequately combine differences in position and orientation at the same time is difficult. Furthermore, the interpretation of such a measurement is not intuitive. Therefore, the differences in position and orientation are considered separately. 
The positional difference is computed by the euclidean distance between the centers of the predicted and the actual object. The difference in orientation is given by the absolute difference between the predicted and the actual orientation.

Each training and testing setting is repeated 20 times and the average differences in position and orientation as well as their standard deviations are reported. 

\subsection{Evaluated configurations}
%TODO
The implementations of both concepts offer several possible configurations, beginning by using different features, to using hard coded components in the gating concept and choosing different learning rates for the underlying regression and classification models.

In this evaluation, the implementations are evaluated first in the configuration described in the last chapter with meta parameters chosen as indicated by table \ref{tab:parameters}

\begin{table}
	\centering
	\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} c c c }
			\hline \textbf{Parameter} & \textbf{Interaction} & \textbf{Gate} \\ 
			\hline \hline 
			 $\sigma$ & 0.05 & 0.05 \\
			 $\eta_{max}$ & 0.001 & 0.001 \\  
			\hline 
	\end{tabular*} 
	\caption{Table summarizing the meta parameters used for the presented evaluations.}
	\label{tab:parameters}
\end{table}

\subsection{Results}

\textbf{Generalization on random training data:}

%TODO if another object is added, specify which one is used here
The first test evaluates the prediction accuracy against the number of random training runs. The training run start positions were chosen at random in the range of \m{-0.25} to \m{0.25}. This means that they span the entire width of the block objects.
For each number of training runs that were evaluated, 21 test runs were performed with starting positions from \m{-0.35} to \m{0.35} at \m{0.035} intervals. This means that 6 test runs went past the object without interacting with it.
The averaged results for the block predictions over all folds for the interaction state model in the first configuration can be seen in figure \ref{fig:learnCurveInteraction1}. The averages over all test runs as well as only over the 15 central test positions are shown.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{LearningCurveinteractionModel_C_0E20FoldsSigma005.pdf}
\caption{Learning curve for the interaction concept in the first configuration. Difference in position and orientation for the block predictions and differences for the actuator position at the end of a run are shown. Error bars indicate one standard deviation. Blue error bars represents averages over all 21 test positions, while red error bars only averages over the 15 test positions where the block is actually moved.}
\label{fig:learnCurveInteraction1}
\end{figure}

The positional prediction error starts around \m{0.24} with a standard deviation of around \m{0.15} when the model is only trained with a single training example. The prediction performance improves to roughly \m{0.09} with a standard deviation of around \m{0.05}.
The predicted orientation error starts at an average of about \rad{0.8} with standard deviation of \rad{0.5} but improves to around \rad{0.25} with standard deviation of \rad{0.09} when increasing the number of random training examples.
The performances for position and orientation do not increase much more after the first 10 training runs (\m{0.12} $+/-$ \m{0.08} for position and \rad{0.33} $+/-$ \rad{0.16} for orientation).
The actuator position predictions remain fairly constant with errors around \m{0.03} with a standard deviation of \m{0.02} for all training scenarios.

When only considering the testing positions where the actuator actually interacts with the block (red error bars), predicted errors improve slightly.

The results for the same experiment with the object state with gating function model using the first configuration are presented in figure \ref{fig:learnCurveGate1}.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{LearningCurvegateModel_C_0E20FoldsAllGateFSigma005.pdf}
\caption{Learning curve for the object state with gating function concept. Difference in position and orientation for the block predictions and differences for the actuator position at the end of a run are shown. Error bars indicate one standard deviation. Blue error bars represents averages over all 21 test positions, while red error bars only averages over the 15 test positions where the block is actually moved.}
\label{fig:learnCurveGate1}
\end{figure}

The positional prediction error over all testing positions (blue error bars) starts at around \m{0.22} with a standard deviation of \m{0.15} for only one training run in the object state with gating function model. By increasing the number of random training runs to 30, the positional error reduces continuously to around \m{0.04} with standard deviation of only around \m{0.03}.
The predicted orientation error starts at around \rad{0.89} with standard deviation of \rad{0.68}.
After 30 test runs an error of \rad{0.10} with a standard deviation of \rad{0.08} remains.
For this model the performance increases only slightly with more training runs after having seen only 10 training runs for both the error in position (\m{0.05} $+/-$ \m{0.04}) as well as orientation (\rad{0.18} $+/-$ \rad{0.12}).
Predictions for the actuator position are fairly constant around \m{0.002} with a standard deviation of around \m{0.0004}.

Unlike in the other model, prediction errors are slightly worse for the block position and orientation when only considering the 15 testing positions where an actual interaction takes place. Actuator prediction are slightly more accurate however.

Memory-based regression methods such as \gls{nn} approaches often suffer from the increasing computational complexity as the number of training samples increases.
Table \ref{tab:learnCurveGateNodes} shows the average number of update calls and number of nodes in the \glspl{aitm} used by the object state model after training for the different number of training runs. Similarly, table \ref{tab:learnCurveInteractionNodes} presents similar numbers for the interaction state model. Since the number of learned \glspl{ac} varies between the runs, only the number of nodes for the \gls{ac} with the most nodes is recorded. 
Only 149 update calls are made at most for each run because the models are not updated after the 150th timestep.


\begin{table}
\footnotesize 
	\centering
	\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} c c c c c}
			\hline \textbf{Train runs} & \textbf{Object Updates}&  \textbf{Object Nodes} & \textbf{Gate Updates} &\textbf{Gate Nodes} \\ 
			\hline \hline 
			 1 & 93.5 & 26.55 & 149 & 3.7 \\
			 2 & 194.35 & 39.4 & 298 & 5.55 \\  
			 3 & 287.25 & 67.4 & 447 & 6.4 \\
			 5 & 479.6 & 111.95 & 745 & 6.8 \\
			 10 & 963.7 & 219.65 & 1490 & 7.75 \\
			 20 & 1894.25 & 438.2 & 2980 & 10.4 \\
			 30 & 2883.8 & 603.05 & 4470 & 10.2 \\
			\hline 
	\end{tabular*} 
	\caption{Record of the average number of nodes that resulted from the recorded number of update calls in the regression and classification \gls{aitm} for the object state model.}
	\label{tab:learnCurveGateNodes}
\end{table}

Interestingly, the number of nodes required for the gating function is rather constant whereas the local forward model in the predictor creates a node between every 4th and 5th update call on average. 

\begin{table}
\footnotesize 
	\centering
	\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} c c c c c}
			\hline \textbf{Train runs} & \textbf{AC Updates}&  \textbf{AC Nodes} & \textbf{ACS Updates} &\textbf{ACS Nodes} \\ 
			\hline \hline 
			 1 & 93.5 & 26.55 & 149 & 3.7 \\
			 2 & 194.35 & 39.4 & 298 & 5.55 \\  
			 3 & 287.25 & 67.4 & 447 & 6.4 \\
			 5 & 479.6 & 111.95 & 745 & 6.8 \\
			 10 & 963.7 & 219.65 & 1490 & 7.75 \\
			 20 & 1894.25 & 438.2 & 2980 & 10.4 \\
			 30 & 2883.8 & 603.05 & 4470 & 10.2 \\
			\hline 
	\end{tabular*} 
	\caption{Record of the average number of nodes that resulted from the recorded number of update calls in the regression and classification \gls{aitm} for the interaction model. Only the number of the \gls{ac} with the most nodes is recorded.}
	\label{tab:learnCurveInteractionNodes}
\end{table}

\textbf{Generalization on selected training data:}

In order to analyze the information required for generalization, another training scheme was used. This time the models were only provided with three predetermined training runs. The starting positions \m{-0.18}, \m{0.0} and \m{0.18} were used in this experiment. These three were chosen in order to provide the models with examples of both possible rotations as well as the lateral movement. The same 21 test runs were performed as before. This time prediction errors at the end of the run are recorded for each starting position separately. Due to the noise explained above, some randomness is still present in the data which is why this experiment was also repeated 20 times and the average errors are reported. 

The results for the interaction model are shown in figure \ref{fig:eachPosInteraction}.

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{EachPositioninteractionModel_C_8E20FoldsAllACSFSigma005.pdf}
\caption{Prediction errors for the interaction model at each of the 21 test positions after seeing three exemplary training examples. The training positions are highlighted by the block lines. Error bars indicate one standard deviation.}
\label{fig:eachPosInteraction}
\end{figure}

As mentioned in the previous setting, the predictions for the test runs that do not actually interact with the block, are rather poor. The model predicts an incorrect change in position around \m{0.18} +/- \m{0.01} and in orientation around \rad{0.77} +/- \rad{0.02} on both sides of the object. 
Predictions are quite good close to the seen training examples when the block is actually influenced: The positional error is as low as \m{0.02} +/- \m{0.01} around the two outer training positions and slightly higher for the central position (\m{0.03} +/- \m{0.02}). Generally the same applies for orientation, but the central position has a prediction error for orientation of around 0, while the outer positions are slightly higher (\rad{0.01} +/- \rad{0.01}).
The further away from the training positions, the worse the prediction for both position and orientation becomes. Interestingly, the worst positional prediction of \m{0.07} +/- \m{0.02}  happens \m{0.105} away from the center, while the worst prediction for orientation of \rad{0.45} +/- \rad{0.01} is seen \m{0.07} away from the center.

It is also interesting to note, that the position and orientation are predicted very consistently (most standard deviations below \m{0.02} and all below \rad{0.02}).

The results for the same experiment performed with the object state model are presented in figure \ref{fig:eachPosGate}.

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{EachPositiongateModel_C_8E20FoldsAllGateFSigma005.pdf}
\caption{Prediction errors for the object state model at each of the 21 test positions after seeing three exemplary training examples. The training positions are highlighted by the block lines. Error bars indicate one standard deviation.}
\label{fig:eachPosGate}
\end{figure}

The object state with gating function successfully does not make predictions for the test runs that go past the object after seeing the three fixed training runs. However, no predictions are also made for the first interacting test positions on either side which results in constant errors in position of \m{0.14} and in orientation of \rad{0.58}.
For the other test positions that actually interact with the block, the predictions are better for positions close to the training examples as in the other model.
The predicted errors are not quite as symmetric as in the other model.
The central position has a prediction error of close to 0 with a standard deviation below \m{0.01}. 
The predictions for position are slightly more accurate between the left and the middle training position (ranging from \m{0.01} +/- \m{0.0} to \m{0.04} +/- \m{0.02}) when compared to the testing positions between the middle and the right training position (ranging from \m{0.01} +/- \m{0.0} to \m{0.07} +/- \m{0.03}). Outside the training positions, the predictions are better for the 17th testing position than for the fifth one (\m{0.09} +/- \m{0.04} at the fifth and \m{0.06} +/- \m{0.01} at the 17th testing position).

Prediction errors for orientation show a similar trend: 
In the central position the prediction error in orientation is \rad{0.04} +/- \rad{0.02}.
The same outliers for orientation are found as in the other model. The testing positions \m{0.07} away from the center have errors around \rad{0.45} +/- \rad{0.04}
The remaining errors for orientation in between the training positions are ranging from \rad{0.06} +/- \rad{0.04} to \rad{0.16} +/- \rad{0.05}.
As with the positional prediction, the prediction for orientation is slightly worse for the fifth testing position compared to the 17th one, although both have the same distance to the closest training position (\rad{0.31} +/- \rad{0.14} at the fifth and \rad{0.28} +/- \rad{0.02} at the 17th testing position).

In order to be able to interpret these results, figure \ref{fig:EachPosEndPos} shows a final configurations (predicted and actual) for every 2nd testing positions for both models.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{EachPosEndPos.png}
\caption{Predicted and actual end positions. Transparent objects represent the predictions. Top row shows results for the interaction model, while bottom row shows the results for the object state with gating function model. Every 2nd of the 21 testing positions is shown. The training positions are right next to the third image on either side as well as between the two central images.}
\label{fig:EachPosEndPos}
\end{figure}

\textbf{Generalization in the open loop:}

While the previous scenarios test the accumulated prediction error, it is often not required to make accurate predictions 150 steps into the future. Instead it is more important to adapt to changes in the environment quickly. For that reason, a variation of the \textit{Push Task Simulation} was also evaluated. In this variation, the models are not trained before testing, but will instead continuously receive updates from the environment. That way the models online learning capabilities are evaluated. The models will still make consecutive predictions based on their last prediction. Their last predictions are not corrected based on the actual environment and the models do not get or compute any feedback as to how good their last prediction was. 

A qualitative evaluation is provided in figure \ref{fig:pushTaskSim2}. The figure shows images of consecutive interactions of the object state model with the environment. The transparent object represent the last predictions, while the darker objects represent the actual states of the objects. In order to be able to actually see meaningful differences between consecutive interactions, these images where taken in a setting where the model is only updated every 10Hz, meaning that the model was updated and queried every 0.1 seconds in simulation time instead of the 0.01 seconds used for all the other evaluations. No other adaptation to the model was made. The top row shows the first run the model performs while the second row shows the following run. The testing positions were predefined.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{pushTaskSim2.png}
\caption{Consecutive interactions in the \textit{Push Task Simulation} variation. Top row shows images of the first test run, while the second row shows images of the second test run on the other side of the object. }
\label{fig:pushTaskSim2}
\end{figure}

The leftmost images show the initial configuration at the start of both runs. While no prediction is made for the actuator in the 2nd image of the top row, the actuator is predicted correctly in the 2nd run. The top row continues to lag one frame behind with its predictions. The final block predictions are accurate again.
In the 2nd run, the actuator predictions are good. However, the model predicts the same rotation as in the first run once the actuator comes close to the object. After a few other frames, the predictions start turning in the other direction, but do not catch up to the actual objects.


\subsection{Extension to multiple objects \label{sec:multipleObjects}}

Environments rarely only contain a single object, however, so far in this thesis, only one object was considered. While it is difficult for the interaction model to handle multiple objects due to the reasons explained in section \ref{sec:interactionTheory}, the object state model handles objects separately anyways. In this evaluation the red block described in table \ref{tab:environmentObjects} is added to the scene and six fixed (three interacting with each object) training positions are used. 
A fixed set of 21 testing positions ranging from \m{-0.8} to \m{0.4} in intervals of \m{0.06}.

The model was not altered in any way for the results presented in figure \ref{fig:eachPosTwoObjects}.

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{EachPosgateModel_C_8E2ObjectsV2.pdf}
\caption{Prediction errors for the object state model with two objects. The training positions are highlighted by the black lines. Error bars indicate one standard deviation.}
\label{fig:eachPosTwoObjects}
\end{figure}

The gating function often classifies incorrectly for the red object, but the blue object does not seem to be influenced by the new object at all since the results are basically identical to the ones presented in \ref{fig:eachPosGate}.

By introducing a seventh training position in between both objects, improves the prediction quality due to better performance of the gating function as can be seen in figure \ref{fig:eachPosTwoObjects7Trains}.


\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{EachPosgateModel_C_8E20Folds2Objects7Train.pdf}
\caption{Prediction errors for the object state model with two objects with an additional separating training positions. The training positions are highlighted by the black lines. Error bars indicate one standard deviation.}
\label{fig:eachPosTwoObjects7Trains}
\end{figure}


\section{Move to Target \label{sec:moveToTarget}}

The \textit{Move to Target} task is designed to evaluate the concept's ability to reach a given target configuration incrementally. 

\subsection{Scenario description}

In this task only a target configuration for the block object is given to the models. At each update step the models are updated with the current worldstate.
This means that they can adapt their inverse model during testing.
Afterwards, the interface queries the model for the next action primitive which is then send to the simulation. This is repeated until the target configuration is reached or a maximum number of steps of 3000 has been performed.
The models consider a target configuration as reached, if the norm of the difference vector between the target representation and the current situation is smaller than 0.01. This means, that the combined vector of positional difference as well as difference of orientation must have a norm smaller 0.01. The models do not know about the oscillation of the orientation feature, i.e. that an orientation of $-\pi = \pi$ which can lead to false negatives.

The models are first trained on a fixed set of training runs. The set is designed to show important interactions to the models before testing.

The training runs are performed by placing the actuator in a suitable starting position before applying a constant action primitive directed towards the block.
The used action primitives all have a norm of 0.5m/s.

After training, both the actuator and the block are returned to their initial position. The block is located \m{0.25} above the global origin without any rotation, while the actuator is placed directly on top of the global origin.
Figure \ref{fig:moveToTargetScenario} visualizes the situation where the actuator just starts moving after training. The transparent light blue block symbolizes the target configuration of the block.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{MoveToTargetScenario.png}
\caption{Visualization of the \textit{Move to Target} task after training. The transparent light blue block indicates the desired target configuration. The blocks have been marked with the black corner to distinguish their orientation.}
\label{fig:moveToTargetScenario}
\end{figure}

\subsection{Evaluation criteria}

The performance of the inverse model is measured by the number of actions required to reach the target. If the target has not been reached within the allowed number of steps, the remaining difference is recorded. Similar to the previous task, position and orientation differences are recorded separately.

While recording the number of required steps, actual interactions where the block is moved and steps where only the actuator moves are distinguished.

Furthermore, the average change in position and orientation per performed action is reported in order to judge the quality of the selected action primitives.

The models are tested with four targets, each translated the same distance from the starting position but with different orientations. Figure \ref{fig:targetPositions} shows the four different target configurations. All target configurations have the same positional difference to the starting position (\m{0.6} in either x direction and \m{0.7} in either y direction) but different orientations: 
\begin{itemize}
\item Target 1: \rad{-2.1}
\item Target 2: \rad{0.75}
\item Target 3: \rad{0.0} 
\item Target 4: \rad{3.1415}
\end{itemize}
For each target, the model is trained separately on the same training set in order to ensure comparability between the targets.

The testing of each target configuration is repeated 5 times and the averages over these runs are reported where possible.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{MoveToTargetTargets.png}
\caption{Visualization of the four target configurations for the \textit{Move to Target} task. The black corners indicate the block's rotation.}
\label{fig:targetPositions}
\end{figure}

\subsection{Results}
The models were first evaluated after seeing the eight training runs visualized in figure \ref{fig:moveToTargetTraining}. These training runs ensure that the inverse models can learn reasonable preconditions for all relevant feature changes.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{MoveToTargetTraining.pdf}
\caption{Visualization of the eight training positions for the \textit{Move to Target} task. The circles indicate the relative starting positions of the actuator, while the arrows indicate the used action primitive.}
\label{fig:moveToTargetTraining}
\end{figure}

The results for the interaction models are presented in table \ref{tab:moveToTargetInteractionResults} while the results for the object state with gating function model are presented in table \ref{tab:moveToTargetGateResults}.

\begin{table} %USE V2 sigma 0.05
	\centering
	\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} c c c c c } %c c}
			\hline \textbf{Target} & \textbf{Reached} & \textbf{Pos} & \textbf{Ori} & \textbf{\# steps} \\%& \textbf{Avg pos} & \textbf{Avg ori} \\ 
			\hline \hline 
			 Target 1 & 3 & 0.55 & 0.002 & 2689.2 (2482) \\ %&  & \\
			 Target 2 & 4 & 0.01 & 0.023 & 2430.4 (2288) \\ %& &  \\  
			 Target 3 & 5 & - & - & 1377.6 \\ %& &  \\  
			 Target 4 & 2 & 0.55 & 2.04 & 2492.2 (1730.5) \\ %& &  \\  
			\hline 
	\end{tabular*} 
	\caption{Evaluation results for the \textit{Move to Target} task for the interaction model. Pos and Ori columns represent the remaining error in position ([m]) and orientation [rad] respectively for the runs that did not reach the target. The number of steps in brackets represents the average over the runs that reached the target.}
	\label{tab:moveToTargetInteractionResults}
\end{table}

The object state with gating function reaches the targets in most cases within the allowed number of steps. 
%TODO redo with hopefully decent data ...
No target is never reached, but the interaction model had a few problems reaching the first and fourth targets. Looking at the combined error of position and orientation for the 3 runs that did not reach the Target 4 in figure \ref{fig:moveToTargetInteractionT4Detail} reveals, that 2 of these runs were actually quite close to the target.

Target 1 appears to be more difficult for the object state model as the average number of steps is significantly higher then for the other target configurations.


\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{MoveToTargetInteractionT1Detail.pdf}
\caption{Detailed view of the three runs towards Target 1 that did not reach the target in the interaction model. The errors in position and orientation as well as the combined error over the interaction steps are shown.}
\label{fig:moveToTargetInteractionT1Detail}
\end{figure}

The marked run for Target 4 in the interaction model is shown in greater detail in figure \ref{fig:moveToTargetInteractionT4Detail}. According to the combined error, the target was reached after around 1500 steps, but the model did not detect that and moved the block away from the target position again.
\begin{table}
	\centering
	\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} c c c c c } %c c}
			\hline \textbf{Target} & \textbf{Reached} & \textbf{Pos} & \textbf{Ori} & \textbf{\# steps} \\ %&  \textbf{Avg pos} & \textbf{Avg ori} \\ 
			\hline \hline 
			 Target 1 & 4 & 0.01 & 0.002 & 2271 (2088.8) \\ %& & \\
			 Target 2 & 5 & - & - & 1695 \\ %& &  \\  
			 Target 3 & 5 & - & - & 1147.4 \\ %& &  \\  
			 Target 4 & 5 & - & - & 1597.4 \\ %& &  \\  
			\hline 
	\end{tabular*} 
	\caption{Evaluation results for the \textit{Move to Target} task for the gate model. Pos and Ori columns represent the remaining error in position ([m]) and orientation [rad] respectively for the runs that did not reach the target. The number of steps in brackets represents the average over the runs that reached the target.}
	\label{tab:moveToTargetGateResults}
\end{table}




