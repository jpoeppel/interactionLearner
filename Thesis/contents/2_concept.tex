\chapter{Concept}

In order to achieve the desired results, multiple models were 
developed incrementally. This chapter explains the different 
concepts in general. Details on the concrete realizations of these models can 
be found in chapter \ref{chap:modelReal} while their evaluation and discussion 
can be found in chapters \ref{chap:evaluation} and \ref{chap:conclusion} 
respectively. %TODO consider removing if it is written at the end of the 
%introduction
The first concept, explained in section \ref{sec:pairInteractions}, uses a 
pairwise interaction representation. In contrast to that, section 
\ref{sec:objectChange} describes the second concept, which represents the 
individual objects and models the change in their dynamics.

\section{Memory based models\label{sec:mbm}}
%TODO do I need such a section and if, should it be here?
TODO? Vl besser bei technologies?

The general idea of memory based learning, also called instance based learning, is to use lazy 
generalization. This means that the system compares a new problem instance with previously seen 
training examples in order to make a prediction instead of using an explicit generalization that 
was computed during training. 

The simplest memory based approach is to simply store the training data as 
input-output pairs in memory. When querying the model, for example in order to 
make a prediction, a nearest neighbor search \cite{nearestNeighbor} is 
performed on the inputs for all seen training examples and the corresponding 
output of the most similar pair is return as the prediction.

The advantages of such an approach are that there is no catastrophic forgetting during online 
learning, since the model only changes locally around each newly inserted data 
point. Furthermore, the required amount of training data is very limited as 
long as it is very similar to the test situation. In fact, depending on the 
query task, a single trained input-output pair can be enough to provide a 
reasonable prediction. 

The disadvantages are increasing query times as the model grows since most of the work is performed then. On top of that, the simple model does not generalize well in between training points if there are big changes. (TODO show graph to visualize this) %TODO add image showing this?

Multiple approaches have been proposed in order to reduce the gravity of the 
disadvantages such as using index structures to increase nearest neighbor 
search or using different methods in order to smooth the model output, for 
example by interpolating between the closest k points. 
%Section \ref{sec:ITM} 
%goes into more details about the underlying memory based structure that was 
%used for this thesis. 
%TODO maybe remove this here, since ITM is not really the memory based model, 
%just part of it/the predictor -.-

A memory based approach was mainly chosen due to the good online training 
capabilities and the one shot learning properties for this thesis.

The different concepts presented here try to structure and cluster the training 
data in order to guide training and improve prediction.

\section{Pairwise interactions\label{sec:pairInteractions}}
%TODO remove/rename word episode or explain it properly!. The concept is 
%independent of the timeframe

The first developed model adopted the commonly used idea of modeling object 
interaction by concentrating on pairwise interactions, also used by 
\cite{pairwise interaction}. The idea is to not model each object for itself 
but rather model the interaction between them by using features that describe 
their relative attributes (see section \ref{sec:pairStates} for details about 
the chosen representation). 
When predicting the next object state, the next interaction state is predicted 
first and the resulting object states are extracted from there afterwards. The 
input for model consists of the current interaction features between two 
objects and the chosen action. The output represents the change from the input 
in the next state.

In order to reduce the query costs the model tries to cluster similar 
interactions in more abstract collections, that are called \enquote{abstract 
episode collections} here. The general idea is visualized in figure \ref{fig:pairIdea}.

\begin{figure}
%TODO make image
\caption{TODO Visualization of pairwise concept}
\label{fig:pairIdea}
\end{figure}


\subsection{Abstract episode collections}

In order to avoid the nearest neighbor search on all recorded episodes, this 
model tries to cluster the experiences it made into more abstract collections.
The clustering is performed based on the set of attributes that changed within 
each episode. The set $S$ of attributes that changed is defined as follows 
\begin{equation}
S = \{f | f \in F ~ \wedge ~ ||Pre(f)-Post(f)|| > \epsilon_{Noise}\}
\end{equation}
where $F$ denotes the set of all features, $Pre(f), Post(f)$, the value of 
feature $f$ in the initial and resulting state of the episode respectively and 
$\epsilon_{Noise}$ a threshold to cancel out small noise.

For each different set the model creates a new abstract collection $AC_i$. The 
idea is that these collections correspond to different interaction scenarios. 
The set $\{spos\}$ for example corresponds to the interaction of pushing the 
object directly in the center, since the distance and relative velocities do not 
change between the two objects. The set $\{dpos, dist\}$ on the other hand 
describes the interaction of the actuator moving without contact to the object. 
The abstract collection effectively split the interaction space in subregions. 

\subsection{Prediction}

Prediction is performed by following a winner-takes-all approach:
%the idea of mixture of experts 
%\cite{mixtureExperts}: 
When predicting, the model first estimates the collection that is most likely 
responsible for the next episode and uses the collection to make the 
prediction. 

By training a classifier, e.g. a decision tree \cite{DT}, on all the 
input-collection pairs, that were already seen, this selection becomes 
independent of the number of training examples already stored. The total number 
of collections is limited to the size of the superset of $F$, although in 
practice, not all possibilities are likely and only those collections are 
considered, that are already made up of more than $\epsilon_{min}$ training 
examples. 
This threshold is used to reduce the number of outliers when training the 
classifier.  

After the suitable collection has been selected, one could perform nearest 
neighbor search on the episodes of that collection in order to make the 
prediction. However, since the number of episodes can grow quite rapidly, each 
collection trains it's own local predictor. 
%TODO if you put something like this here, make sure you show results that %support something like 
%this: 
%By splitting the entire interaction space into 
%subregions, each local predictor can be much simpler than one model that tries 
%to predict the entire space.
This also allows the model to remove stored episodes when the local predictors 
are already trained sufficiently well. 

In order to reduce the burden on the manual feature and metric selection, an 
automatic feature selection can be performed periodically in each abstract 
collection in order to improve the performance of the local predictors. %TODO 
%show comparison in results

\subsection{Planning}
%TODO
(TODO consider redoing the planning algorithm more similar to the way it is done
in the most recent model, which works, maybe the same ideas can be employed 
here)

In order to get a suitable action given a target configuration, the model first computes the 
difference set between the current situation and the target 
configuration. The model then searches for an abstract episode collection that 
corresponds to the same set of attributes and delays the action computation to 
that collection. In case the model does not yet now of such an collection, the 
most similar collection is chosen instead, meaning the collection that covers 
most of the changed attributes in the computed difference set. 

The abstract collection queries it's own inverse model, which can be the same 
as their predictor, if that model supports bidirectional queries, for the 
action that produces the output most similar to the desired target. 

[TODO FORMEL] %TODO

The model can then use it's forward model in order to check if the selected 
action is applicable in the current situation and if it indeed reduces the 
distance to the target. In case no suitable action could be selected, the model 
can choose to perform a random action, in order to experience new interactions, 
that will improve the following predictions and planning results.


\subsection{Theoretical problems}

The biggest problem with the pairwise interaction based approach is that it 
does not model the interactions between multiple objects. In a scenario with 
two objects alongside the actuator, it is not trivial to decide which pairwise 
interaction should be used to predict the next state of each of the objects, 
even if only two entities interact. In interaction scenarios where all three 
objects are touching at least one of the other objects, the pairwise 
representation fails completely since it can only represent effects between two 
of them at any given time. 

\begin{itemize}
\item AC selection
\item Number of samples increases a lot (even without interaction/touch, the interaction states 
differ -> one AC gets trained)
\begin{itemize}
\item The most simple AC (no interaction, just actuator movement) becomes the slowest
\end{itemize}
\end{itemize}


\section{Object states and global changes\label{sec:objectChange}}
(TODO
Maybe actually drop this concept or briefly mention it as intermediate step to the current, best 
working model?)


In order to avoid the problem of multiple objects, a second concept was 
developed. Instead of pairwise interactions the objects are modeled 
individually by object states (see section \ref{sec:objectStates} for details about the used representation and features). 
Predictions are made solely based on \enquote{local} features for each object. 
Local means here, that an object predicts e.g. it's next position only based on it's
current features, such as position and velocity. 
In order to be able to predict the effects of interactions such as collisions, 
another layer of predictors is introduced that predict the \enquote{global} 
changes on the dynamics of each object. Here global means, that other objects 
are used to compute the used features.

Overall the main difference is that instead of predicting the next state 
directly, by predicting the next interaction state, first only the dynamic 
features are predicted and later based on that, the remaining features are 
computed. This concept is visualized in figure \ref{fig:osChangeIdea}.

\begin{figure}
%TODO make image visualising the idea
\caption{TODO Visualization of object states + global change}
\label{fig:osChangeIdea}
\end{figure}


\subsection{Local prediction}

As mentioned above, instead of predicting an interaction and extracting the 
object states, the object states are predicted directly based on their \enquote{local} features. In order to avoid strong differences between different object types, one predictor is trained for each type from the corresponding episodes. 

\subsection{Global change prediction}

During an interaction, such as a collision, the local attributes of each 
involved object change. Since these changes depend on the \enquote{global} 
properties of the objects relative to each other, they are called global change 
here. 
In order to be able to predict these changes, these relative attributes need to 
be considered. This model uses pairwise interactions again, similar to the 
previous concept. Since the actual object states do not need to be recovered 
from these interaction states different features can be used.

One can now train one predictor that predicts the changes to the local features 
of each object based on a given interaction vector. This predictor would need 
to be able to generalize over the entire interaction space and over all objects.
Alternatively, one can train one of these predictors for each object.

Another approach yet again, is to employ a similar strategy as before:
By splitting the interaction space into subregions and training a local predictor for each region, 
these local predictors can be simpler and more specialized. This however, entails the problem of 
actually splitting the interaction space.... %TODO name different/used options
 

\subsection{Prediction}

In order to predict the next object states, the model first considers each 
object pair and checks for global changes based on these interaction states.
Selecting the appropriate change is done similar to the abstract collection 
section mentioned in the previous model. A classifier is trained using the 
interaction state and change pairs that were experienced before. 
That way an object's attributes can be changed by multiple interactions which 
allows also interaction scenarios with more than two objects involved. 
Afterwards each object predicts it's next state based on it's local, 
potentially modified, features.

(There is a serious flaw in this argumentation, that kind of negates the 
purpose of this concept. By still using only interaction states, the changes 
might come from multiple objects but it makes a big difference if only one 
objects pushes one other, or multiple...)

\subsection{Planning}
%TODO
(TODO if at all, this has not been tried yet with this concept. Either make 
this one to the current one or omit?)

\subsection{Theoretical problems}

%TODO 
\begin{itemize}
\item Approach is not really correct for multiple objects since dynamics change differently 
depending on if there is only one object involved or multiple
\item Action selection is not trivial
\item Action separation (gold standard) is quite hard to do. 
\item Predicting changes is not really suitable, since dynamics also change from untracked 
interactions (friction) 
\end{itemize}

\section{Object states with gate function}
%TODO make properly

[TODO only rough draft]

Both previous models separated the space of possible interactions into different cluster. The first
concept by introducing the abstract collections and the second one by (potentially) introducing different
change predictors. In both cases, is it necessary to train the model on training data from all possible situations,
including those, were only the actuator moves without interacting with any object. This does not only reduce the 
performance of the model since there are more references to search through when making predictions, but it can also hinder
the selection classifier, since it will usually see a lot more training examples for this \enquote{no interaction} scenario.
The evaluations (see chapter \ref{chap:evaluation} for details) show that the accuracy of the predictions greatly depend
on the accuracy of the selection mechanism. 

In order to avoid this problem, a \enquote{gate function} is added in this concept. This gate function is used to determine for 
every object besides the actuator if in a given situation a selected action will have an effect on that object.
If the gate function predicts that there will be an effect, the change is predicted by a predictor.

This again creates two layers for prediction, as the other concepts did (compare figure \ref{fig:osGateIdea} with \ref{fig:pairIdea} and \ref{fig:osChangeIdea}). 
The main difference here is however, that this gate function
can be a lot simpler than the AC selection since it is only a binary classifier. In fact, this gate function can even be provided
to the model for specific domains, where such knowledge is available since it does not depend on internal structuring as the AC selection does.

The predictor predicts the objects directly as in the second concept but not only on local features, but uses features from the interaction
of the actuator and the object that is predicted to change. 

The gate function can also be used between objects in case multiple objects are present in the scene. In that case interactions between
objects that do not involve the actuator can be handled as well.

(STILL TO TRY) In order to predict multi object interactions, the gate function can be used to first collect all involved objects before
invoking the appropriate predictor. (e.g. by interpolating the different input features).


\begin{figure}
\caption{TODO Visualization of object state + gate}
\label{fig:osGateIdea}
\end{figure}

\subsection{Planning}

Planning is performed fairly similar to the idea mentioned in the pairwise concept: First a check if the target has already been reached is made.
In case the target has not been reached the predictor is ask to provide the required conditions that need to be met in order to move the target objects towards the target
configuration. These conditions are provided by the inverse model of the predictor.
In this case however, a special inverse model is used instead of using the inverse model of a bidirectional regression model. Searching for specific target changes in the
trained regression model has several disadvantages: Firstly, the distance to the target configuration can potentially be a lot greater than any changes the model as seen during training.
In fact this will be the case in most cases when the can not reach the target by executing a single action. This would require the inverse model to generalize greatly to unknown regions.
Secondly, the target configuration can consist of different attributes, such as position and orientation. Depending on the used regression model, the accuracy of the model might depend on the quality of a metric combining such different domains.

Therefore, instead of relying on the quantity of the target change, only the direction of each attribute is considered in this special inverse model. For each output change direction, the inverse model trains a prototype during training. This prototype computes the average preconditions for its given change. 
This means that there might be a prototype responsible for a positive change in the local x position of the object.
This prototype will collect preconditions for this change, that the model has experienced during training, in order to construct an average precondition that produces such a change.

When asked for preconditions in order to reach a certain target configuration, the inverse model uses a greedy strategy to provide the preconditions that reduces the attribute with the
biggest difference. 

The returned preconditions are then analyzed if they can be executed given the current situation. If they can be executed, the contained actuator action simply returned. 
Otherwise, the model tries to find an action that moves the actuator to a configuration closer to the given preconditions without interacting with the object on the way.



