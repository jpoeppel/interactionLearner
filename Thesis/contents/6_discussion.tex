\chapter{Discussion \label{chap:discussion}}

The last chapter presented the results of the evaluations of both developed concepts. Overall it can be argued that both concepts fulfill the stated goal of incrementally learning about simple object interactions. This chapter discusses the results of the evaluations in more detail as well as discuss the developed underlying mechanism in the \gls{aitm} and the \gls{tiim}.

\section{Generalization performance}

The first task was designed to evaluate the generalization capabilities of the memory-based concepts. It is important to note, that no domain knowledge outside the stated assumptions was provided to either model for this prediction task. Although the used features were selected by hand, no preprocessing of these features was performed and no knowledge about special feature dimensions was provided. It is highly likely, that the generalization performance of both models could be increased by providing fine tuned distance metrics and/or preprocessing the used features.
However, as this kind of knowledge is usually not available to the machine when encountering novel situations, such optimizations were not considered here.

\subsection{Generalization on random training data}

Prediction performances of both models reach acceptable levels for position and orientation considering 150 timesteps are predicted without correction. 
Both models' prediction for the blocks position are on average less then 10 cm incorrect after having seen 20 random training runs. The gating model even requires only 5 random training runs for this accuracy. 
The models predict the rotation of the objects correct up to an average error of around 15° for the interaction, and less then 6° for the gating model.

Overall, the gating model outperforms the interaction state model by requiring less training runs while achieving better prediction results as soon as at least 2 training runs have been seen.

This performance difference is most likely due to the higher complexity of the \acrfull{acs} compared to the gating function. While the gating function only needs to learn a binary classifier, the Abstract Case Selector, needs to choose from a potentially big number of possible local models. Choosing an incorrect model usually results in an inaccurate prediction, which will be accumulated over the course of the test run. Furthermore, the gating function is provided with more information than the Abstract Case Selector, due to the features distance and closing, that are introduced in the relative interaction features. 

The better performance of the interaction model after just one training run, is most likely due to the fact, that the gating function needs at least two runs before it works sufficiently well. An incorrect choice by the \gls{acs} which predicts a change in the object might still result in a better prediction than predicting no change at all. 

The worse performance in predicting the actuator position of the interaction model was expected and is due to the problem of dependency within an interaction state, as already mentioned in section \ref{sec:interactionTheory}. In the interaction model, the actuator is not predicted directly, but only in combination with the block. That way the actuator's prediction is not only inaccurate if the regression model is not sufficiently trained, but also when the \gls{acs} chooses an incorrect \gls{ac}. 

The dedicated regression model in the object state concept does not suffer from these problems and can learn the direct relation between an action primitive and the resulting change in the actuator's state. Furthermore, the evaluation setting makes it very easy for the dedicated forward model since only a constant input is used in order to predict an output that is constant up to the noise explained in section \ref{sec:noise}. This also shows in the fact that only the minimum number of two nodes are learned in the forward model for the actuator. The fact that the prediction performance of the local model is as good as it is indicates that the additional input constraint in the \gls{aitm} is working well to filter out the noise. Adapting the winning node in the case that the predicted output is not good enough when the given input is too close to the winning node, reduces the number of required nodes while also averaging out noise in the data.

The learned number of nodes is concerning when considering lifelong learning. Especially the number of nodes required in the \gls{acs} and in the object specific \gls{aitm} show a concerning trend of scaling linearly with the number of interactions.
While the model managed to finish almost all\footnote{During the experiment, some runs took slightly longer than 0.02 seconds due to fluctuations in the systems load. However, the number of these cases was far below 0.1\% in all experiments and the introduced inaccuracy in the data was considered small additional noise. } update and query calls during the experiments within the allowed time of 0.02 seconds, increasing numbers of nodes are bound to impact the models performance in the long run. The intermediate solution would be to implement the models in a more efficient language as well as perform optimizations for the involved computations. To solve this problem, the generalization performance of the \gls{aitm} would need to be improved or different regression methods would need to be used. 

\subsection{Generalization on selected training data}

The evaluations with selected training runs highlight to what extend the proposed models are capable of generalization. Obviously for memory based approaches, the best performance is achieved close to the known positions. However, despite not having seen the situations with the biggest change in orientation, the models are able to interpolate to some extend as can be seen in figure \ref{fig:EachPosEndPos}.

Overall the gating model performs better than the interaction state model, which is most likely attributed to the already mentioned discrepancy in complexity between the gating function and the \gls{acs}. 
However, around the edges of the object, the interaction state model makes better predictions about the orientation. It turns out, that the gating model does not make predictions for the outer testing positions. In this case, the gating function is not sufficiently trained with the provided data.
Despite that, the gating function is able to generalize to unseen situations where the object is not influenced by the actuator. Although the \gls{acs} does know at least one \gls{ac} where only the actuator is moving, it is not able to learn the correct selection on the provided training samples.
Adding another training run, that passes the object on one side results in correct predictions for the testing runs on that side, but usually not on the other one.

This behavior is to be expected since the memory-based classifier works on similarities rather than correlations. In order to make a prediction, the two most similar known cases are consulted. Since no feature preprocessing or adapted metric is used, the distance in the classifier's input space determines the prediction. The additional features of distance and closing appear to provide enough additional information to the gating model to be able to generalize beyond the distance in the 2d space of the objects' position. Without these two features, the gating concept performs similar to the interaction concept.
However, simply adding equivalent features to the interaction state model is not guaranteed to produce better results as it further increases the dimensionality of the feature vector and thus of the input space the regression model needs to learn. Furthermore, the computation of these two features requires knowledge about the object's shape which needs to be provided to the gating model, but is not required for the interaction state model.

It is interesting, that both models require less nodes in their \glspl{aitm} when being provided with more structured training data when compared to the random training runs.
The most reasonable explanation for this phenomenon is that the selected training positions result in less dynamic interactions. For example, the positions that produce the biggest change in orientation are not included in these training runs, but are likely to have appeared in random training example.  


\subsection{Generalization with constant feedback} 

The images in figure \ref{fig:pushTaskSim2} showcase the one-shot learning capabilities of the memory-based regression and classification models. While both models lag one timestep behind in the first run, they predict changes in the actuator in the second run correctly.
The predictions for the block are already quite good in the first run because the models reach the block in the predictions only after they were already updated by the actual interaction. Despite the fact that the rotations are predicted one frame later then in reality due to the late start for the actuator, the actual block predictions are accurate.

The interactions in the 2nd row of the figure are interesting.
Since the actuator predictions for both models are accurate, the models have never seen the interaction at the right side of the block before it comes to predicting it. Therefore, it predicts the rotation from the situation that is closest to the current situation. In this case, this results in the wrong prediction for orientation.
Furthermore, an impossible configuration of actuator and block is predicted where the actuator is located within the block. 
The models do not know about physical laws and perform no checks for such situations.
However, in the interaction state model the actuator is predicted to be slightly further outside, resulting in less overlap with the block prediction. 
In the next frames, the models adapt their forward models to the new information that is being provided and start rotating the predicted block in the other direction. The gating model cannot completely compensate the relatively big error.
This is because the gating function stops to predict an influence of the predicted actuator on the predicted block.
In the interaction state model, the predicted block keeps turning for additional update steps before the \gls{acs} starts selecting an \gls{ac} that does not change the block anymore.
While the final prediction of the interaction state model might be closer to the actual block, the gating model managed to learn the interaction more precisely. This is because the gating function correctly predicted that the predicted actuator should not influence the predicted block anymore in the rightmost image of the second row.
The already mentioned difference in complexity between the gating function and the \gls{acs} is also apparent in this evaluation.

Nevertheless, the images show the one-shot capabilities one would expect from memory-based approaches for both models. If a higher update rate is used, such as 100Hz as in the other experiments, smaller prediction errors will be made. In that case the currently predicted situation remains close to the actual situation, which means that the models can still apply what they have just learned for its next prediction.


\subsection{Extension two multiple objects}

Normal environments contain far more than only a single object. While the robot might perform a detection of a single salient object \cite{salient}, it generally should be able to deal with multiple objects in its environment. The focus of this thesis was to incrementally learn the interaction with one object, however the results in section \ref{sec:multipleObjects} show that the developed object state with gating function model is capable of dealing with multiple objects without being adapted.
While the interaction state model suffers from the decision problem between multiple alternative predictions as discussed in section \ref{sec:interactionTheory}, the gating model does not need to be adapted in order to successfully make predictions for different types of objects. Since the performance of the blue block is not influenced by adding another object to the environment it is likely, that the gating model can deal with an arbitrary number of objects.

However, the results in figure \ref{fig:eachPosTwoObjects} indicate a problem with the gating function. More precisely with the \gls{aitm} when it is used for (binary) classification. The \gls{aitm} relies on the output error when deciding to add a new node. In the binary classification case, this output will always only be one of two values (0 or 1). The result of that is, that the first nodes that are inserted as prototypes for either case are used to cover a large input space. 
If not sufficient amounts of diverse training samples are seen, this will lead to poor performances. Ideally, the node deletion scheme would remove nodes far away from the decision border over time, but this does not seem to be the case in this scenario. Adding the seventh training run in between both objects introduced an additional required node in the gating function, which greatly improves the prediction performance for the red block as can be seen in figure \ref{fig:eachPosTwoObjects7Trains}.


Unfortunately, the object state concept is not able to make predictions about object-object interactions. Although, the general idea for object-object interactions is explained in the concept's description (see section \ref{sec:gatePrediction}), the problem of determining the correct causal relation in order to train an appropriate gating function was not solved over the course of this thesis.

\section{Reaching target states}

The second task was designed in order to evaluate the learned inverse models of the concepts. Both concepts essentially use the same underlying inverse model, which was designed for the challenges that the incremental learning approach entails. 
For that reason, this section can also be understood as a discussion of the developed inverse model.

The inverse model itself is not provided with any domain knowledge, but does make the assumption about averaging feature dimensions. 
However, it was necessary to provide the models with information about what some feature dimensions represent in order to successfully reach a given target configuration. 
Furthermore, it was required to provide a circling action in order to avoid moving blocks arbitrarily.

Both models reach the target configuration most of the time. Target 1 appears to be more difficult for both models compared to the other targets, since it is not always reached and the average number of required steps is significantly higher for the first target than for the other targets. 
Looking at the test runs for Target 1 in more detail in figure \ref{fig:moveToTargetInteractionT1Detail} revealed that the \gls{tiim} often selected suboptimal preconditions in the cases, where the target was not, or just barely reached.
Two possible reasons for this poor performance come to mind:
\begin{enumerate}
\item The merging process produces suboptimal alternatives within the nodes.
\item The network selects the worse of two provided alternatives.
\end{enumerate}

1) Due to numerical inaccuracy as well as noise, some suboptimal combinations can become dominant within a prototype. In this case, the optimal combinations might be lost during the merging process. The likelihood for this is reduced by the stricter merging process described in section \ref{sec:invModelRealization}, but not completely removed. The mentioned problem of \enquote{loosing zero dimensions} appears to be part of the problem in the third and fifth runs shown in figure \ref{fig:moveToTargetInteractionT1Detail}. After the difference in orientation has been successfully reduced by the model, it tries to reduce the positional distance. In order to do that the model would need to push the block straight in the center, i.e. the preconditions should indicate the actuators relative position to be around 0, but apparently it pushes slightly away from the center which results in a new difference in orientation. This indicates that the optimal combination was lost in the merging process. The shown run 4 does not suffer from this problem as it manages to reduce the positional distance without impacting the orientation.

2) The greedy strategy of the network compares the two alternative preconditions for the currently biggest feature dimension to the alternatives of the second biggest feature dimension. The alternative that is closest to either of the second alternatives is chosen. This decision was motivated by the idea of planning ahead for the next feature dimension in order to avoid unnecessary circling movements around the objects.

In case some of these alternative are suboptimal (e.g. due to the first problem), the network might choose a bad precondition, even if the other alternative would have been optimal. That is because the suboptimal preconditions might be closer to the preconditions for the second biggest feature dimension. 
Furthermore, not all feature dimensions in the precondition should be considered when looking at closeness. For example, the preconditions for both models contain the relative actuator velocity (in the interaction state model this is implicitly given in the included action primitive). 
Different interactions require different velocities in order to produce the desired changes in the object's state. 
However, since the velocity of the actuator can be changed directly, it should not be considered when trying to choose the precondition that is closest to the preconditions required for the next feature. 
Preprocessed features or a fine tuned distance metric for preconditions would certainly reduce this problem, but is not available in the given setting as this would provide additional prior knowledge.

Figure \ref{fig:moveToTargetInteractionT1Detail} points out another problem with the used strategy of the network: In order to avoid oscillating between different feature dimensions, the one with the biggest difference is initially chosen and reduced until its sign changes or until the difference is considered small enough. While this does successfully avoid oscillations, it makes the model more vulnerable to errors in the merging process. As can be seen in the figure, the model creates bigger differences in orientation than what was given in the starting configuration while trying to reduce the difference in position. This indicates that the network should pay more attention to the development of other feature dimensions while reducing one.

Another thing to note is that the interaction concept generally requires more steps in order to reach the target than the object state concept. Careful analysis of the testing runs revealed that the used circling decision plays a big part in this. The decision, if the model should employ a circling action or not, depends on the distance to the location determined by the preconditions. In case the actuator simply needs to move to the other side of a corner, no circling action is employed but the actuator is moved directly in the direction towards the location. In this case the actuator would push at the corner of the object. While the gating model uses the gating function in order to determine if this action is safe in the sense that it does not move the object, the interaction state model does not have such a feature. Therefore, the block is often moved in an undesired way which needs to be corrected by additional interactions.

One problem that did not manifest itself visibly in the evaluation lies in the use of circular features such as the orientation. The angle of orientation is given in the range of $[-\pi,\pi]$. The model does not know that turning above $\pi$ results in negative orientations. It also does not know that the orientation $3.14$ and $-3.14$ are actually very close. When computing the difference vector $\vec{d}$, such a scenario will result in a big difference which the model tries to reduce. In most cases by turning the object essentially by almost 360°. This problem can only be overcome by providing additional information about the feature dimensions or preprocessing features in a way that differences behave the same for all feature dimensions.

On the positive side, the proposed inverse model does not suffer from the general problem of memory-based approaches of becoming computational expensive the more training data they receive. This is because the \gls{tiim} stores only a nearly constant amount of values in order to compute the averages for each feature dimension. The number of possible sign combinations that are stored separately is limited by the size of the superset of sign combinations. Furthermore, the inverse model successfully allows to deduce action primitive suitable to reduce the distance to a given state. This is also true for distances far greater than anything the model has seen during training. In that sense, the proposed inverse model is capable of enormous extrapolation.

The inverse model does not provide an action plan to reach a given target. Actual action sequences would need to be planned by higher level components of the robot. 
However, the provided forward and inverse model should provide the required tools in order to formulate plans successfully.
In a sense, the models currently do just that by analyzing the returned preconditions, defining intermediate targets for the actuator and computing a path (circling) in order to reach these intermediate targets without collision. Due to the interactive nature of this setting, these plans are never kept for more than a single update step but rather recomputed each time an action primitive is queried in order to adapt to new situations.

\section{The Adapted Instantaneous Topological Mapping \label{sec:discAITM}} 

Both developed implementations use the same underlying regression and classification model in the form of the \gls{aitm}. Being an adaptation of the \gls{gng} and similar in its output to a \gls{knn} it does suffer from the typical problem of memory-based approaches when it comes to its dependence on the used metric and unnormalized features.

Both implementations do not really adapt the created nodes within the network, but rather rely on the topological mapping for the most part. The only adaptation they use is in the additional check of the input distance when deciding to insert a new node. 
The main reason for not using learning rates to adapt already learned nodes, is that the learning rates would need to be fine tuned to the given situation. Since the idea of this thesis is to develop and test models that adapt to unknown situations, reducing the number of tuned meta parameters is desired. In fact, having to choose the parameters listed in table \ref{tab:parameters} is already questionable when considering the original goal of autonomous self adaption.

Another reason for not using learning rates, especially for the matrix $A$, is that it can require a lot of training iterations before a stable matrix is found. In order not to influence early predictions, the matrix needs to be initialized as a zero matrix of suitable dimensions. Depending on the learning rate and the changes in the data, updating a zero matrix can easily lead to instabilities.
Since the models constantly rely on the regression method, it is better to use a constant output per node instead of adding noise through an incorrect linear interpolation. Especially, since the method already interpolates between the output of two nodes.
In scenarios where more training data and more prior knowledge is available the amount of required nodes is likely to decrease significantly when using positive learning rates.

The presented results of the required nodes indicate, that the \gls{aitm} in its current form does not generalize well enough for lifelong learning. Ideally the relative number of required nodes should decline with increasing amounts of training data, however this was not the case at least for the first 30 training runs. In order to reduce the number of required nodes, the area of influence of each node needs to be increased. While the linear interpolation matrix $A$ was designed to do that, the problem of reliably training this matrix incrementally with limited amounts of data remains. While outside the scope of this thesis, learning rates that are adapted automatically to the given data might provide a solution to this problem.

Apart from that, the experiment using two objects highlighted some problems with the \gls{aitm} when used for classification. The dependence on the output alone without adapting the underlying input nodes can result to poor generalization. This indicates that the current update strategy for the \gls{aitm} should be improved in the future.

Nevertheless, combined with the proposed models the \gls{aitm} is able to make fairly accurate predictions about a complex environment without preprocessing any features or using special distance metrics. Furthermore, the method was also used successfully as a classifier in both the binary and the general case. 
Only the output interpolation needs to be turned off in order for the identical method to work for classification instead of regression. Considering the initial goal of self adaptation to unknown environments, such a universal method is very useful. Since the decision if output interpolation should be used or not can be changed at any time by the model, it is theoretically possible to make that decision automatically.

\section{Concept comparison}

This thesis provides two different concepts in order to incrementally learn simple object manipulations. The main differences between the concepts are summarized in table \ref{tab:comparison}.

\begin{table}
	\footnotesize
	\centering
	\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} c c c}
			\hline  & \textbf{Pairwise interaction} & \textbf{Object state} \\
			\hline \hline 
			 World representation & Pairwise interaction states & Individual object states  \\ 
			 Actuator representation & Only part of an interaction state & Explicitly modeled \\
			 Action primitive & Influence any interaction state & Influences the actuator \\
			 Subspace creation & Changing feature sets & Object groups \\
			 Interaction separation & None & Gating function \\
			 Prediction & Simultaneously & Subsequently starting at actuator \\
			\hline 
	\end{tabular*} 
	\caption{Summary of the main differences of the two developed concepts.}
	\label{tab:comparison}
\end{table}

Both concepts use the same memory-based regression and classification methods for the actual predictions and mainly differ in the way represent their environment. The pairwise interaction concept uses pairwise interaction states between two objects in order to represent both objects together. The object state with gating function concept on the other hand represents the objects individually and uses these to compute pairwise features when required.

Both concepts split the state space they encounter into local subspaces. While this is biologically inspired \cite{kawato1999internal}, the main reason was to reduce the burden on the trained regression models and allow for quicker update and query times.

The gating concept uses a gating function in order to differentiate between interactions that influence another object and those that do not. This greatly reduces the complexity of the local models for each object group. The findings of Johansson et al. also provide some evidence for a similar concept in humans, by highlighting the importance of contact events \cite{johansson2001eye}. The predictions of the gating function can be interpreted as contact predictions in this context.

The gating concept is tailored more specifically to the given situation of object manipulation by explicitly representing the actuator. The interaction state concept on the other hand does not even know of different objects for prediction. This knowledge needs to be provided when trying to reach target states.

In that sense, the interaction state concept is better suited for the initial problem of automatically adapting to unknown environments because it makes less assumptions in its structure about the possible environments. 
However, the gating concept's assumption about separating the actuator explicitly might be reasonable in the context of robotics, where the robot should at least be provided with basic knowledge about itself. 

The results in the previous chapter show that both concepts were able to learn about the dynamics of a simple unknown environment without additional information provided by humans, which was the main goal of this thesis.

